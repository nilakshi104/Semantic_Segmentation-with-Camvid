{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Camvid segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJZgedMrj+NL/+4W+53tpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilakshi104/Semantic_Segmentation-with-Camvid/blob/master/Camvid_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBYhbhqibwuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "121c6d03-4c82-4b76-b131-eadf22f1a578"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbwCtmCQpDNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "418f23dc-06dd-4a3a-8457-969107ffae74"
      },
      "source": [
        "pip install segmentation-models-pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: efficientnet-pytorch>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch) (0.6.3)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch) (0.6.1+cu101)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet-pytorch>=0.5.1->segmentation-models-pytorch) (1.5.1+cu101)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.41.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet-pytorch>=0.5.1->segmentation-models-pytorch) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ojm0AF_cI43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5vclskPcMBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataroot = '/content/drive/My Drive/Untitled folder/Camvid dataset/'\n",
        "ckptroot = '/content/drive/My Drive/Untitled folder/Camvid dataset/checkpoint_val/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvMpJ5ME7fRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnVkZoj0cP27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "outputId": "99f06fa8-b047-4c9e-bb35-ca3732ccc9a2"
      },
      "source": [
        "k=cv2.imread('/content/drive/My Drive/Untitled folder/Camvid dataset/train/0001TP_006690.png')\n",
        "print(k.shape)\n",
        "k_l=cv2.imread('/content/drive/My Drive/Untitled folder/Camvid dataset/train_labels/0001TP_006690_L.png')\n",
        "print(k_l.shape)\n",
        "cv2_imshow(k_l)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(720, 960, 3)\n",
            "(720, 960, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=960x720 at 0x7FCAF1D4C160>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21mTeL4ucUDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(folder):\n",
        "    images = [];\n",
        "    for filename in os.listdir(folder): # List all the filenames in the folder\n",
        "        img = cv2.imread(os.path.join(folder, filename)); # Join the link of the folder and filename\n",
        "        img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA);\n",
        "        images.append(img);\n",
        "    return images\n",
        "\n",
        "# To load labeled gray-scaled images\n",
        "\n",
        "def load_label_img(folder):\n",
        "    images = [];\n",
        "    for filename in os.listdir(folder): # List all the filenames in the folder\n",
        "        img = cv2.imread(os.path.join(folder, filename)); # Join the link of the folder and filename\n",
        "        img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA);\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY);\n",
        "        images.append(img);\n",
        "    return images"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU96wPJKcdRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "af6650dd-b0c7-42de-a8a4-21e35942b0c7"
      },
      "source": [
        "labels_name= np.loadtxt(dataroot+'labels1.txt',delimiter='\\n',dtype=str);\n",
        "labels_name"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n",
              "       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n",
              "       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n",
              "       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n",
              "       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n",
              "       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'TunnelVegetation',\n",
              "       'Misc', 'Void', 'Wall'], dtype='<U17')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkMyylXcjr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors=[]\n",
        "def c2g(cn):\n",
        "    cn = np.reshape(cn, (1, 1, 3));\n",
        "    cn = cv2.cvtColor(cn, cv2.COLOR_BGR2GRAY);\n",
        "    return cn;\n",
        "colors.append(c2g(np.array([64, 128, 64], dtype = 'uint8')));# opencv supports uint8 and not int32/int64\n",
        "colors.append(c2g(np.array([128, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 0, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 192, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 64, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 192, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 64, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 64, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 192, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 192, 64], dtype = 'uint8')));\n",
        "\n",
        "def class_pixel(label_img):\n",
        "    class_pix = np.ones([256,256, 1], dtype = int);\n",
        "    for index, c in enumerate(colors):\n",
        "        class_pix[label_img == c] = index; # Vectorized masking is much much faster\n",
        "    return class_pix\n",
        "\n",
        "\n",
        "def label_img_list(img_list):\n",
        "    images = [];\n",
        "    for image in img_list:\n",
        "        images.append(class_pixel(image));\n",
        "    return images;\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IR1h-7nco84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "transform_img = transforms.Compose([ \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.409, 0.4, 0.38],   # input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "                        std=[0.27, 0.27, 0.26])])\n",
        "\n",
        "transform_img_label = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "# We have to create custom classes in order to use the DataLoader. These classes inherit the Dataset class\n",
        "\n",
        "class trainset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_train = None, root_train_label = None, transform_label = None):\n",
        "        self.train_img = load_img(root_train);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.train_label_img = label_img_list(load_label_img(root_train_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.train_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "      \n",
        "        img = self.transform(self.train_img[index]);\n",
        "        label = self.transform_label(self.train_label_img[index]);\n",
        "        return img, label;\n",
        "\n",
        "class valset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_val = None, root_val_label = None, transform_label = None):\n",
        "        self.val_img = load_img(root_val);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.val_label_img = label_img_list(load_label_img(root_val_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.val_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.val_img[index]);\n",
        "        label = self.transform_label(self.val_label_img[index]);\n",
        "        return img, label\n",
        "\n",
        "    \n",
        "class testset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_test = None, root_test_label = None, transform_label = None):\n",
        "        self.test_img = load_img(root_test);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.test_label_img = label_img_list(load_label_img(root_test_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.test_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.test_img[index]);\n",
        "        label = self.transform_label(self.test_label_img[index]);\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# traindataset = trainset(transform_img,dataroot+'train/',dataroot+'train_labels/' , transform_img_label);\n",
        "testdataset = testset(transform_img,dataroot+'test/',dataroot+'test_labels/' , transform_img_label);\n",
        "# valdataset = valset(transform_img,dataroot+'val/',dataroot+'val_labels/' , transform_img_label);"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AN7jmysfl-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=2\n",
        "batch_size1=5\n",
        "num_workers=1\n",
        "# train_loader = data.DataLoader(traindataset,batch_size= batch_size , shuffle=True,  num_workers=num_workers);\n",
        "test_loader = data.DataLoader(testdataset,batch_size= batch_size1 , shuffle=True,  num_workers=num_workers);\n",
        "# val_loader = data.DataLoader(valdataset ,batch_size= batch_size , shuffle=True,  num_workers=num_workers);"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZW9Jlwq4ftG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES = ['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n",
        "       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n",
        "       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n",
        "       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n",
        "       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n",
        "       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'TunnelVegetation',\n",
        "       'Misc', 'Void', 'Wall']\n",
        "       \n",
        "BACKBONE='resnet34'\n",
        "n_classes=len(CLASSES)\n",
        "activation='softmax2d'               #as ACTIVATION = None for logits or 'softmax2d' for multiclass segmentation\n",
        "model = smp.Unet(BACKBONE, classes=n_classes, activation=activation)\n",
        "model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3rFsYWZgGBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8947afc-b3ec-4a5d-fc1a-00dff32c9e6e"
      },
      "source": [
        "# model=torchvision.models.resnet34(pretrained=True)\n",
        "# for param in model.parameters():\n",
        "#   param.requires_grad=False\n",
        "# newmodel=torch.nn.Sequential(*(list(model.children())[:-2]))\n",
        "# newmodel.conv= nn.Sequential(\n",
        "#             nn.Conv2d(512, 1024, kernel_size=(3,3), padding=(1,1),stride=(1, 1)),\n",
        "#             nn.BatchNorm2d(1024),\n",
        "#             nn.Conv2d(1024, 32, kernel_size=(1, 1), stride=(2, 2), bias=False),\n",
        "#             nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "# )\n",
        "\"\"\"\n",
        "for finetuning guide:\n",
        "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\"\"\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor finetuning guide:\\nhttps://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH4SeO-MdJ33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs=50\n",
        "# start_epoch=0\n",
        "# lr=1e-5\n",
        "# wd=1e-7\n",
        "# optimizer = optim.Adam(model.parameters(),\n",
        "#                        lr=lr,\n",
        "#                        weight_decay=wd,\n",
        "#                        betas = (0.99, 0.9),\n",
        "#                        eps = 1e-7)\n",
        "# criterion = nn.CrossEntropyLoss();\n",
        "# scheduler = MultiStepLR(optimizer, milestones=[20,40], gamma=0.1)\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# def toDevice(datas, device):\n",
        "#     imgs, labels = datas\n",
        "#     return imgs.to(device), labels.to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2kn2TChhHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "  'Trainer'\n",
        "  def __init__(self,\n",
        "               ckptroot,\n",
        "               model,\n",
        "               devices,\n",
        "               epochs,\n",
        "               criterion,\n",
        "               optimizer,\n",
        "               scheduler,\n",
        "               start_epoch,\n",
        "               train_loader,\n",
        "               val_loader\n",
        "               ):\n",
        "      super(Trainer,self).__init__()\n",
        "\n",
        "      self.model=model\n",
        "      self.device=device\n",
        "      self.epochs=epochs\n",
        "      self.ckptroot=ckptroot\n",
        "      self.criterion=criterion\n",
        "      self.optimizer=optimizer\n",
        "      self.scheduler=scheduler\n",
        "      self.start_epoch=start_epoch\n",
        "      self.train_loader=train_loader\n",
        "      self.val_loader=val_loader\n",
        "\n",
        "  def train(self):\n",
        "      # trainloss=[]\n",
        "      # valloss=[]\n",
        "      trainloss1=[]\n",
        "      valloss1=[]\n",
        "      # self.model.to(self.device)\n",
        "      for epoch in range(self.start_epoch,self.epochs+self.start_epoch):\n",
        "        print(epoch)\n",
        "        train_loss=0\n",
        "        self.model.train()\n",
        "        for local_batch,train_img  in enumerate(self.train_loader):\n",
        "          # train_img=toDevice(train_img,self.device)\n",
        "          self.optimizer.zero_grad()\n",
        "          imgs,labels=train_img\n",
        "          output=self.model(imgs)\n",
        "          # print(output.size())\n",
        "          outpt=output.type(torch.LongTensor)\n",
        "          loss=self.criterion(output,labels.squeeze(1))\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          train_loss +=loss\n",
        "        \n",
        "          # if local_batch % 25 ==0:\n",
        "          #   print(f\"Training Epoch: {epoch} | Loss: {loss} | Training_Loss: {train_loss/(local_batch+1)}\")\n",
        "\n",
        "        # trainloss.append(train_loss/(local_batch+1))\n",
        "        trainloss1.append(train_loss)\n",
        "\n",
        "        self.scheduler.step()\n",
        "        self.model.eval()\n",
        "        valid_loss=0\n",
        "        with torch.set_grad_enabled(False):\n",
        "          for local_batch,val_img in enumerate(self.val_loader):\n",
        "              # val_img=toDevice(val_img,self.device)\n",
        "              self.optimizer.zero_grad()\n",
        "              imgs,labels=val_img\n",
        "              output=self.model(imgs)\n",
        "              outpt=output.type(torch.LongTensor)\n",
        "              loss=self.criterion(output,labels.squeeze(1))\n",
        "              valid_loss += loss\n",
        "              # if local_batch % 25 ==0:\n",
        "                # print(f\"Val Epoch: {epoch} | Loss: {valid_loss / (local_batch + 1)}\")\n",
        "\n",
        "          # valloss.append(valid_loss/(local_batch+1))\n",
        "          valloss1.append(valid_loss)\n",
        "          \n",
        "          # Save model\n",
        "        if epoch==30 or epoch==40 or epoch>48 :\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                'input_size':3,\n",
        "                'output_size':1,\n",
        "                # 'hidden_layers':[each.out_features for each in model.hidden_layers],\n",
        "                'loss':loss\n",
        "            }\n",
        "            self.save_checkpoint(state)\n",
        "            print(\"==> Save checkpoint ...\")\n",
        "\n",
        "      plt.subplot(2,1,1)\n",
        "      plt.plot(trainloss1)\n",
        "      plt.subplot(2,1,2)\n",
        "      plt.plot(valloss1)\n",
        "\n",
        "\n",
        "  def save_checkpoint(self, state):\n",
        "    \"\"\"Save checkpoint.\"\"\"\n",
        "    print(\"==> Save checkpoint ...\")\n",
        "    if not os.path.exists(self.ckptroot):\n",
        "        os.makedirs(self.ckptroot)\n",
        "\n",
        "    torch.save(state, self.ckptroot + 'camvid segmentation model:-{}.h5'.format(state['epoch']))\n",
        "           "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO2RGg2Zja1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"==> Start training ...\")\n",
        "trainer = Trainer(ckptroot,\n",
        "                  model,\n",
        "                  device,\n",
        "                  epochs,\n",
        "                  criterion,\n",
        "                  optimizer,\n",
        "                  scheduler,\n",
        "                  start_epoch,\n",
        "                  train_loader,\n",
        "                  val_loader)\n",
        "trainer.train()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1V37GRrSbJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" using trained parameters stored in h file in model \"\"\"\n",
        "\n",
        "filepath='/content/drive/My Drive/Untitled folder/Camvid dataset/checkpoint_val/camvid segmentation model:-41.h5'\n",
        "checkpoint=torch.load(filepath)\n",
        "BACKBONE='resnet34'\n",
        "CLASSES = ['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n",
        "       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n",
        "       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n",
        "       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n",
        "       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n",
        "       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'TunnelVegetation',\n",
        "       'Misc', 'Void', 'Wall']\n",
        "n_classes=len(CLASSES)\n",
        "activation=None\n",
        "model = smp.Unet(BACKBONE, classes=n_classes, activation=activation)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch=checkpoint['epoch']\n",
        "scheduler=checkpoint['scheduler_state_dict']\n",
        "state_dict=checkpoint['model_state_dict']\n",
        "# print(state_dict)\n",
        "# model.eval()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUQiyfPdmWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in scheduler['_last_lr']:\n",
        "#   k=i\n",
        "# lr=k\n",
        "# wd=1e-7\n",
        "# criterion = nn.CrossEntropyLoss();\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# def toDevice(datas, device):\n",
        "#     imgs, labels = datas\n",
        "#     return imgs.to(device), labels.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz2C_KrVvVcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96b6da28-21c7-4eb3-d3fd-1720273497af"
      },
      "source": [
        "name2id = {v:k for k,v in enumerate(labels_name)}\n",
        "void_code=name2id['Void']\n",
        "\n",
        "def corr(x): # To get proper correspondence between the outputs and the labels\n",
        "    # x = x.cpu();\n",
        "    x = x.detach().numpy(); # Detach() was used as one can't convert a pytorch tensor to a numpy array if required_grad is set True for that variable\n",
        "    x = x.argmax(axis = 1);\n",
        "    return x;\n",
        "\n",
        "# # Compute test-set accuracy\n",
        "\n",
        "correct = 0;\n",
        "total = 0;\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, labels = data;\n",
        "    outputs = model(inputs);\n",
        "    outputs = corr(outputs);\n",
        "    labels = labels.detach().numpy();\n",
        "    # outputs = outputs.squeeze(1)\n",
        "    outputs=np.squeeze(outputs)\n",
        "    # labels=labels.squeeze(1)\n",
        "    labels=np.squeeze(labels)\n",
        "    for i in range(256):\n",
        "      for j in range(256):\n",
        "          total = total + 1;\n",
        "          if labels.any() != void_code:\n",
        "            if (outputs[1, i, j] == labels[1, i, j]):\n",
        "              correct = correct + 1;\n",
        "          else:\n",
        "            correct=correct+1\n",
        "            print('void')\n",
        "\n",
        "print(\"Hence, the test set accuracy is \", (correct/total) * 100);"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hence, the test set accuracy is  57.79764512005974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsBJuscYn6kP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "e53e63fa-25d6-421e-bf0f-8f077709634b"
      },
      "source": [
        "colors_rev = [];\n",
        "colors_rev.append(np.array([64, 128, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 0, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 128, 0], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 128, 0], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([0, 0, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 0, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 0, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 128, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 192, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 64, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 0, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 0, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 128, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 0, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 64, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 192, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([0, 64, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 64, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 128, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 0, 0], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 128, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 128, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 128, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 0, 0], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 64, 0], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([128, 64, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([0, 128, 128], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([192, 128, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([64, 0, 64], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([0, 192, 192], dtype = 'uint8'));\n",
        "colors_rev.append(np.array([0, 0, 0], dtype = 'uint8'))\n",
        "colors_rev.append(np.array([0, 192, 64], dtype = 'uint8'));\n",
        "\n",
        "def test(op_img):\n",
        "    class_pix = np.ones([256,256, 3], dtype = 'uint8');\n",
        "    for index, c in enumerate(colors_rev):\n",
        "        class_pix[op_img == index] = c; # Vectorized masking is much much faster\n",
        "    # return class_pix.reshape((85,85, 3))\n",
        "    return class_pix\n",
        "\n",
        "print(labels[0])\n",
        "print(outputs[0])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[21 21 21 ... 21 21 21]\n",
            " [21 21 21 ... 21 21 21]\n",
            " [21 21 21 ... 21 21 21]\n",
            " ...\n",
            " [18 18 18 ... 17 17 17]\n",
            " [18 18 18 ... 17 17 17]\n",
            " [18 18 18 ... 17 17 17]]\n",
            "[[21 26 26 ...  4 21  4]\n",
            " [21 26 26 ...  4 21  4]\n",
            " [26 26 26 ...  4  4  4]\n",
            " ...\n",
            " [19 19 19 ... 17 17 17]\n",
            " [17 17 19 ... 17 17 17]\n",
            " [17 17 17 ... 17 17 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN1ayfuonpAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # outputs=outputs.detach().numpy()\n",
        "# print(type(test(outputs[1])));\n",
        "# a = test(outputs[1]);\n",
        "# a = a.reshape([256,256, 3])\n",
        "# print(a.shape)\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# cv2_imshow(a)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bMHARRnpV5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d=test(labels[1])\n",
        "# cv2_imshow(d)"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}